{
  "cloudProviders": [
    {
      "id": "openai",
      "name": "OpenAI",
      "models": [
        {
          "id": "gpt-5.2",
          "name": "GPT-5.2",
          "description": "Latest flagship reasoning model"
        },
        {
          "id": "gpt-5-mini",
          "name": "GPT-5 Mini",
          "description": "Fast and cost-efficient"
        },
        {
          "id": "gpt-5-nano",
          "name": "GPT-5 Nano",
          "description": "Ultra-fast, low latency"
        },
        {
          "id": "gpt-4.1",
          "name": "GPT-4.1",
          "description": "Strong baseline, 1M context"
        },
        {
          "id": "gpt-4.1-mini",
          "name": "GPT-4.1 Mini",
          "description": "Smaller GPT-4.1 model"
        },
        {
          "id": "gpt-4.1-nano",
          "name": "GPT-4.1 Nano",
          "description": "Lowest latency GPT-4.1"
        }
      ]
    },
    {
      "id": "anthropic",
      "name": "Anthropic",
      "models": [
        {
          "id": "claude-sonnet-4-5",
          "name": "Claude Sonnet 4.5",
          "description": "Balanced performance"
        },
        {
          "id": "claude-haiku-4-5",
          "name": "Claude Haiku 4.5",
          "description": "Fast with near-frontier intelligence"
        },
        {
          "id": "claude-opus-4-5",
          "name": "Claude Opus 4.5",
          "description": "Most capable Claude model"
        }
      ]
    },
    {
      "id": "gemini",
      "name": "Google Gemini",
      "models": [
        {
          "id": "gemini-2.5-pro",
          "name": "Gemini 2.5 Pro",
          "description": "Most capable Gemini model"
        },
        {
          "id": "gemini-2.5-flash",
          "name": "Gemini 2.5 Flash",
          "description": "High-performance with thinking"
        },
        {
          "id": "gemini-2.5-flash-lite",
          "name": "Gemini 2.5 Flash Lite",
          "description": "Lowest latency and cost"
        },
        {
          "id": "gemini-2.0-flash",
          "name": "Gemini 2.0 Flash",
          "description": "Fast, long-context option"
        }
      ]
    },
    {
      "id": "groq",
      "name": "Groq",
      "models": [
        {
          "id": "qwen/qwen3-32b",
          "name": "Qwen3 32B",
          "description": "Powerful reasoning model"
        },
        {
          "id": "llama-3.3-70b-versatile",
          "name": "LLaMA 3.3 70B",
          "description": "Meta's latest versatile model"
        },
        {
          "id": "llama3-70b-8192",
          "name": "LLaMA 3 70B",
          "description": "8K context, balanced performance"
        },
        {
          "id": "llama3-8b-8192",
          "name": "LLaMA 3 8B",
          "description": "Fast and efficient"
        },
        {
          "id": "mixtral-8x7b-32768",
          "name": "Mixtral 8x7B",
          "description": "32K context, mixture of experts"
        },
        {
          "id": "gemma2-9b-it",
          "name": "Gemma 2 9B",
          "description": "Google's efficient model"
        }
      ]
    }
  ],
  "localProviders": [
    {
      "id": "qwen",
      "name": "Qwen",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n",
      "models": [
        {
          "id": "qwen2.5-0.5b-instruct-q5_k_m",
          "name": "Qwen2.5 0.5B",
          "size": "0.4GB",
          "sizeBytes": 429496729,
          "description": "Smallest model, fast but limited capabilities",
          "fileName": "qwen2.5-0.5b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "Qwen/Qwen2.5-0.5B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-1.5b-instruct-q5_k_m",
          "name": "Qwen2.5 1.5B",
          "size": "1.3GB",
          "sizeBytes": 1395864371,
          "description": "Small model, good for basic tasks",
          "fileName": "qwen2.5-1.5b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "Qwen/Qwen2.5-1.5B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-3b-instruct-q5_k_m",
          "name": "Qwen2.5 3B",
          "size": "2.3GB",
          "sizeBytes": 2469606195,
          "description": "Balanced model for general use",
          "fileName": "qwen2.5-3b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "Qwen/Qwen2.5-3B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-7b-instruct-q4_k_m",
          "name": "Qwen2.5 7B",
          "size": "4.7GB",
          "sizeBytes": 5046586241,
          "description": "Large model with high quality (Q4_K_M)",
          "fileName": "qwen2.5-7b-instruct-q4_k_m.gguf",
          "quantization": "q4_k_m",
          "contextLength": 128000,
          "hfRepo": "Qwen/Qwen2.5-7B-Instruct-GGUF",
          "recommended": true
        },
        {
          "id": "qwen2.5-7b-instruct-q5_k_m",
          "name": "Qwen2.5 7B (Q5)",
          "size": "5.4GB",
          "sizeBytes": 5798205849,
          "description": "Large model, high quality reasoning (Q5_K_M)",
          "fileName": "qwen2.5-7b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 128000,
          "hfRepo": "Qwen/Qwen2.5-7B-Instruct-GGUF"
        }
      ]
    },
    {
      "id": "mistral",
      "name": "Mistral AI",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "[INST] {system}\n\n{user} [/INST]",
      "models": [
        {
          "id": "mistral-7b-instruct-v0.3-q4_k_m",
          "name": "Mistral 7B Instruct v0.3",
          "size": "4.4GB",
          "sizeBytes": 4724956928,
          "description": "Fast and efficient instruction model",
          "fileName": "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 32768,
          "hfRepo": "bartowski/Mistral-7B-Instruct-v0.3-GGUF",
          "recommended": true
        },
        {
          "id": "mistral-7b-instruct-v0.3-q5_k_m",
          "name": "Mistral 7B Instruct v0.3 (Q5)",
          "size": "5.1GB",
          "sizeBytes": 5477387264,
          "description": "Higher quality instruction model",
          "fileName": "Mistral-7B-Instruct-v0.3-Q5_K_M.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "bartowski/Mistral-7B-Instruct-v0.3-GGUF"
        }
      ]
    },
    {
      "id": "llama",
      "name": "Meta Llama",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
      "models": [
        {
          "id": "llama-3.2-1b-instruct-q4_k_m",
          "name": "Llama 3.2 1B",
          "size": "0.9GB",
          "sizeBytes": 966367642,
          "description": "Tiny model for edge devices",
          "fileName": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "bartowski/Llama-3.2-1B-Instruct-GGUF"
        },
        {
          "id": "llama-3.2-3b-instruct-q4_k_m",
          "name": "Llama 3.2 3B",
          "size": "2.0GB",
          "sizeBytes": 2147483648,
          "description": "Small but capable multilingual model",
          "fileName": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
          "recommended": true
        },
        {
          "id": "llama-3.1-8b-instruct-q4_k_m",
          "name": "Llama 3.1 8B",
          "size": "4.9GB",
          "sizeBytes": 5260091802,
          "description": "Powerful model with great performance",
          "fileName": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF"
        }
      ]
    },
    {
      "id": "openai-oss",
      "name": "OpenAI OSS",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n",
      "models": [
        {
          "id": "gpt-oss-20b-mxfp4",
          "name": "GPT-OSS 20B",
          "size": "12.1GB",
          "sizeBytes": 12999763968,
          "description": "OpenAI's open-weight model for consumer hardware",
          "fileName": "gpt-oss-20b-mxfp4.gguf",
          "quantization": "mxfp4",
          "contextLength": 128000,
          "hfRepo": "ggml-org/gpt-oss-20b-GGUF",
          "recommended": true
        }
      ]
    }
  ]
}
