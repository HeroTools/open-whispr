{
  "transcriptionProviders": [
    {
      "id": "openai",
      "name": "OpenAI",
      "baseUrl": "https://api.openai.com/v1",
      "models": [
        {
          "id": "gpt-4o-mini-transcribe",
          "name": "GPT-4o Mini Transcribe",
          "description": "Fast and accurate transcription"
        },
        {
          "id": "gpt-4o-transcribe",
          "name": "GPT-4o Transcribe",
          "description": "Most accurate transcription"
        },
        {
          "id": "whisper-1",
          "name": "Whisper",
          "description": "Original Whisper model"
        }
      ]
    },
    {
      "id": "groq",
      "name": "Groq",
      "baseUrl": "https://api.groq.com/openai/v1",
      "models": [
        {
          "id": "whisper-large-v3-turbo",
          "name": "Whisper Large v3 Turbo",
          "description": "216x real-time speed"
        }
      ]
    }
  ],
  "cloudProviders": [
    {
      "id": "openai",
      "name": "OpenAI",
      "models": [
        {
          "id": "gpt-5.2",
          "name": "GPT-5.2",
          "description": "Latest flagship reasoning model"
        },
        {
          "id": "gpt-5-mini",
          "name": "GPT-5 Mini",
          "description": "Fast and cost-efficient"
        },
        {
          "id": "gpt-5-nano",
          "name": "GPT-5 Nano",
          "description": "Ultra-fast, low latency"
        },
        {
          "id": "gpt-4.1",
          "name": "GPT-4.1",
          "description": "Strong baseline, 1M context"
        },
        {
          "id": "gpt-4.1-mini",
          "name": "GPT-4.1 Mini",
          "description": "Smaller GPT-4.1 model"
        },
        {
          "id": "gpt-4.1-nano",
          "name": "GPT-4.1 Nano",
          "description": "Lowest latency GPT-4.1"
        }
      ]
    },
    {
      "id": "anthropic",
      "name": "Anthropic",
      "models": [
        {
          "id": "claude-sonnet-4-5",
          "name": "Claude Sonnet 4.5",
          "description": "Balanced performance"
        },
        {
          "id": "claude-haiku-4-5",
          "name": "Claude Haiku 4.5",
          "description": "Fast with near-frontier intelligence"
        },
        {
          "id": "claude-opus-4-5",
          "name": "Claude Opus 4.5",
          "description": "Most capable Claude model"
        }
      ]
    },
    {
      "id": "gemini",
      "name": "Google Gemini",
      "models": [
        {
          "id": "gemini-2.5-pro",
          "name": "Gemini 2.5 Pro",
          "description": "Most capable Gemini model"
        },
        {
          "id": "gemini-2.5-flash",
          "name": "Gemini 2.5 Flash",
          "description": "High-performance with thinking"
        },
        {
          "id": "gemini-2.5-flash-lite",
          "name": "Gemini 2.5 Flash Lite",
          "description": "Lowest latency and cost"
        },
        {
          "id": "gemini-2.0-flash",
          "name": "Gemini 2.0 Flash",
          "description": "Fast, long-context option"
        }
      ]
    },
    {
      "id": "groq",
      "name": "Groq",
      "models": [
        {
          "id": "qwen/qwen3-32b",
          "name": "Qwen3 32B",
          "description": "Powerful reasoning with thinking mode, 131K context"
        },
        {
          "id": "openai/gpt-oss-120b",
          "name": "GPT-OSS 120B",
          "description": "OpenAI's open-source flagship, 500 T/sec"
        },
        {
          "id": "openai/gpt-oss-20b",
          "name": "GPT-OSS 20B",
          "description": "Fast open-source model, 1000 T/sec"
        },
        {
          "id": "llama-3.3-70b-versatile",
          "name": "LLaMA 3.3 70B",
          "description": "Meta's versatile model, 280 T/sec"
        },
        {
          "id": "llama-3.1-8b-instant",
          "name": "LLaMA 3.1 8B",
          "description": "Ultra-fast 560 T/sec, 131K context"
        },
        {
          "id": "mixtral-8x7b-32768",
          "name": "Mixtral 8x7B",
          "description": "32K context, mixture of experts"
        },
        {
          "id": "gemma2-9b-it",
          "name": "Gemma 2 9B",
          "description": "Google's efficient model"
        }
      ]
    }
  ],
  "localProviders": [
    {
      "id": "qwen3",
      "name": "Qwen3",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n",
      "models": [
        {
          "id": "qwen3-8b-q4_k_m",
          "name": "Qwen3 8B",
          "size": "5.0GB",
          "sizeBytes": 5368709120,
          "description": "Latest Qwen3 with thinking mode support",
          "fileName": "qwen3-8b-q4_k_m.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "Qwen/Qwen3-8B-GGUF",
          "recommended": true
        },
        {
          "id": "qwen3-8b-q5_k_m",
          "name": "Qwen3 8B (Q5)",
          "size": "5.9GB",
          "sizeBytes": 6333399654,
          "description": "Higher quality Qwen3 with thinking mode",
          "fileName": "qwen3-8b-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 131072,
          "hfRepo": "Qwen/Qwen3-8B-GGUF"
        },
        {
          "id": "qwen3-4b-q4_k_m",
          "name": "Qwen3 4B",
          "size": "2.5GB",
          "sizeBytes": 2684354560,
          "description": "Compact Qwen3 with reasoning capabilities",
          "fileName": "qwen3-4b-q4_k_m.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "Qwen/Qwen3-4B-GGUF"
        },
        {
          "id": "qwen3-1.7b-q8_0",
          "name": "Qwen3 1.7B",
          "size": "1.8GB",
          "sizeBytes": 1932735283,
          "description": "Small but capable Qwen3 model",
          "fileName": "qwen3-1.7b-q8_0.gguf",
          "quantization": "q8_0",
          "contextLength": 131072,
          "hfRepo": "Qwen/Qwen3-1.7B-GGUF"
        },
        {
          "id": "qwen3-0.6b-q8_0",
          "name": "Qwen3 0.6B",
          "size": "0.6GB",
          "sizeBytes": 644245094,
          "description": "Tiny Qwen3 for edge devices",
          "fileName": "qwen3-0.6b-q8_0.gguf",
          "quantization": "q8_0",
          "contextLength": 131072,
          "hfRepo": "Qwen/Qwen3-0.6B-GGUF"
        },
        {
          "id": "qwen3-32b-q4_k_m",
          "name": "Qwen3 32B",
          "size": "19.8GB",
          "sizeBytes": 21260251955,
          "description": "Most powerful local Qwen3 with deep reasoning",
          "fileName": "qwen3-32b-q4_k_m.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "Qwen/Qwen3-32B-GGUF"
        }
      ]
    },
    {
      "id": "qwen",
      "name": "Qwen 2.5",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n",
      "models": [
        {
          "id": "qwen2.5-0.5b-instruct-q5_k_m",
          "name": "Qwen2.5 0.5B",
          "size": "0.4GB",
          "sizeBytes": 429496729,
          "description": "Smallest model, fast but limited capabilities",
          "fileName": "qwen2.5-0.5b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "Qwen/Qwen2.5-0.5B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-1.5b-instruct-q5_k_m",
          "name": "Qwen2.5 1.5B",
          "size": "1.3GB",
          "sizeBytes": 1395864371,
          "description": "Small model, good for basic tasks",
          "fileName": "qwen2.5-1.5b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "Qwen/Qwen2.5-1.5B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-3b-instruct-q5_k_m",
          "name": "Qwen2.5 3B",
          "size": "2.3GB",
          "sizeBytes": 2469606195,
          "description": "Balanced model for general use",
          "fileName": "qwen2.5-3b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "Qwen/Qwen2.5-3B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-7b-instruct-q4_k_m",
          "name": "Qwen2.5 7B",
          "size": "4.7GB",
          "sizeBytes": 5046586241,
          "description": "Large model with high quality (Q4_K_M)",
          "fileName": "qwen2.5-7b-instruct-q4_k_m.gguf",
          "quantization": "q4_k_m",
          "contextLength": 128000,
          "hfRepo": "Qwen/Qwen2.5-7B-Instruct-GGUF"
        },
        {
          "id": "qwen2.5-7b-instruct-q5_k_m",
          "name": "Qwen2.5 7B (Q5)",
          "size": "5.4GB",
          "sizeBytes": 5798205849,
          "description": "Large model, high quality reasoning (Q5_K_M)",
          "fileName": "qwen2.5-7b-instruct-q5_k_m.gguf",
          "quantization": "q5_k_m",
          "contextLength": 128000,
          "hfRepo": "Qwen/Qwen2.5-7B-Instruct-GGUF"
        }
      ]
    },
    {
      "id": "mistral",
      "name": "Mistral AI",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "[INST] {system}\n\n{user} [/INST]",
      "models": [
        {
          "id": "mistral-7b-instruct-v0.3-q4_k_m",
          "name": "Mistral 7B Instruct v0.3",
          "size": "4.4GB",
          "sizeBytes": 4724956928,
          "description": "Fast and efficient instruction model",
          "fileName": "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 32768,
          "hfRepo": "bartowski/Mistral-7B-Instruct-v0.3-GGUF",
          "recommended": true
        },
        {
          "id": "mistral-7b-instruct-v0.3-q5_k_m",
          "name": "Mistral 7B Instruct v0.3 (Q5)",
          "size": "5.1GB",
          "sizeBytes": 5477387264,
          "description": "Higher quality instruction model",
          "fileName": "Mistral-7B-Instruct-v0.3-Q5_K_M.gguf",
          "quantization": "q5_k_m",
          "contextLength": 32768,
          "hfRepo": "bartowski/Mistral-7B-Instruct-v0.3-GGUF"
        }
      ]
    },
    {
      "id": "llama",
      "name": "Meta Llama",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{user}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
      "models": [
        {
          "id": "llama-3.2-1b-instruct-q4_k_m",
          "name": "Llama 3.2 1B",
          "size": "0.9GB",
          "sizeBytes": 966367642,
          "description": "Tiny model for edge devices",
          "fileName": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "bartowski/Llama-3.2-1B-Instruct-GGUF"
        },
        {
          "id": "llama-3.2-3b-instruct-q4_k_m",
          "name": "Llama 3.2 3B",
          "size": "2.0GB",
          "sizeBytes": 2147483648,
          "description": "Small but capable multilingual model",
          "fileName": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
          "recommended": true
        },
        {
          "id": "llama-3.1-8b-instruct-q4_k_m",
          "name": "Llama 3.1 8B",
          "size": "4.9GB",
          "sizeBytes": 5260091802,
          "description": "Powerful model with great performance",
          "fileName": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
          "quantization": "q4_k_m",
          "contextLength": 131072,
          "hfRepo": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF"
        }
      ]
    },
    {
      "id": "openai-oss",
      "name": "OpenAI OSS",
      "baseUrl": "https://huggingface.co",
      "promptTemplate": "<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n",
      "models": [
        {
          "id": "gpt-oss-20b-mxfp4",
          "name": "GPT-OSS 20B",
          "size": "12.1GB",
          "sizeBytes": 12999763968,
          "description": "OpenAI's open-weight model for consumer hardware",
          "fileName": "gpt-oss-20b-mxfp4.gguf",
          "quantization": "mxfp4",
          "contextLength": 128000,
          "hfRepo": "ggml-org/gpt-oss-20b-GGUF",
          "recommended": true
        }
      ]
    }
  ]
}
